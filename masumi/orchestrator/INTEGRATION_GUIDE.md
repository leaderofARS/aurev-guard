# ðŸŽ¯ Masumi Orchestrator - AI Training Integration Guide

## Overview

The Masumi orchestrator has been enhanced to fully integrate all AI model training parameters from `agents/ai_model/src/`. This guide explains the new architecture, endpoints, and usage.

---

## âœ¨ What's New

### 1. **Complete Training Parameter Schema** (`ai_training_params.py`)

Comprehensive Pydantic models for all training parameters:

```python
AITrainingConfig  # Master config object
â”œâ”€â”€ TransactionDataParams          # Data loading from Blockfrost
â”œâ”€â”€ DataIOParams                   # I/O caching
â”œâ”€â”€ VolumeFeatureParams            # Time-window features
â”œâ”€â”€ EntropyFeatureParams           # Shannon/RÃ©nyi entropy
â”œâ”€â”€ DailyAggregationParams         # Daily aggregation
â”œâ”€â”€ GraphBuildParams               # Graph construction
â”œâ”€â”€ GraphMetricsParams             # Graph metrics (centrality, clustering)
â”œâ”€â”€ CommunityDetectionParams       # Community detection
â”œâ”€â”€ IsolationForestParams          # IF hyperparameters
â”œâ”€â”€ OneClassSVMParams              # SVM hyperparameters
â”œâ”€â”€ LocalOutlierFactorParams       # LOF hyperparameters
â”œâ”€â”€ AnomalyEnsembleParams          # Ensemble voting
â”œâ”€â”€ RandomForestParams             # RF hyperparameters
â”œâ”€â”€ SHAPExplainerParams            # SHAP configuration
â”œâ”€â”€ SHAPArtifactParams             # SHAP output saving
â”œâ”€â”€ NarrativeExplainerParams       # Narrative generation
â”œâ”€â”€ LivePipelineParams             # Live inference
â”œâ”€â”€ InferenceParams                # Inference modes
â””â”€â”€ PreprocessingParams            # Data preprocessing
```

**All parameters directly map to code in:**
- `agents/ai_model/src/data_pipeline.py`
- `agents/ai_model/src/feature_engineering.py`
- `agents/ai_model/src/graph_features.py`
- `agents/ai_model/src/ml_pipeline.py`
- `agents/ai_model/src/inference.py`
- `agents/ai_model/src/shap_explain.py`
- `agents/ai_model/src/live_pipeline.py`

### 2. **Enhanced Models** (`models.py`)

New Pydantic models for training workflows:

```python
AIModelTrainingStep          # Individual training step
AIModelTrainingPipeline      # Complete pipeline execution
AIModelPredictionRequest     # Prediction request schema
AIModelPredictionResponse    # Prediction response schema
TrainingDataQuality          # Data quality metrics
ModelPerformanceMetrics      # Model evaluation metrics
```

### 3. **AI Model Agent Handler** (`ai_model_agent.py`)

New FastAPI microservice integrating with orchestrator:

**Endpoints:**
- `GET /health` - Service health
- `POST /predict` - Run predictions (Isolation Forest + Random Forest)
- `POST /train/initialize` - Initialize training pipeline
- `POST /train/run/{pipeline_id}` - Execute training
- `GET /train/pipeline/{pipeline_id}` - Get pipeline status
- `GET /config/training` - Get complete config
- `POST /config/training` - Update config
- `POST /data/quality` - Assess data quality

### 4. **Enhanced Router** (`router.py`)

New workflow types:

```python
Workflows:
â”œâ”€â”€ settle              # Payment + risk assessment
â”œâ”€â”€ ai_predict         # Pure AI prediction
â”œâ”€â”€ ai_train           # Initialize training
â”œâ”€â”€ ai_train_run       # Run training pipeline
â”œâ”€â”€ ai_config          # Get/update config
â””â”€â”€ data_quality       # Assess data quality
```

### 5. **Extended App Endpoints** (`app.py`)

**Training Configuration:**
- `GET /masumi/training/config` - Get current config
- `POST /masumi/training/config/apply` - Apply new config
- `GET /masumi/training/config/defaults` - Get defaults

**Training Pipeline:**
- `POST /masumi/training/initialize` - Create new pipeline
- `POST /masumi/training/run/{pipeline_id}` - Start execution
- `GET /masumi/training/pipeline/{pipeline_id}` - Check status

**AI Predictions:**
- `POST /masumi/predict` - Run prediction

**Data Quality:**
- `POST /masumi/data/quality` - Quality assessment

**Statistics:**
- `GET /masumi/stats` - System overview

### 6. **Updated Config** (`config.yaml`)

```yaml
agents:
  - ai_model:
    capabilities: [predict, health, train/initialize, train/run, config/training, data/quality]
    
training:
  data:
    blockfrost_api_key: ${BLOCKFROST_API_KEY}
    max_blocks: 500
  features:
    time_windows: [24h, 7d, 30d]
    include_entropy: true
    include_daily_agg: true
    include_graph: true
  graph:
    compute_centrality: [betweenness, closeness, eigenvector]
  anomaly:
    models: [isolation_forest, one_class_svm, lof]
    contamination: 0.1
  risk_model:
    n_estimators: 200
    max_depth: 20
  explainability:
    explainer_type: tree
    save_artifacts: true

workflows:
  settle, ai_predict, ai_train, ai_train_run, ai_config, data_quality
```

---

## ðŸš€ Usage Examples

### 1. Get Training Configuration

```bash
curl http://localhost:8080/masumi/training/config
```

**Response:**
```json
{
  "status": "success",
  "config": {
    "isolation_forest": {
      "n_estimators": 300,
      "contamination": 0.1,
      "random_state": 42
    },
    "random_forest": {
      "n_estimators": 200,
      "max_depth": 20,
      "class_weight": "balanced"
    },
    "shap_explainer": {
      "explainer_type": "tree",
      "save_artifacts": true
    },
    ...
  }
}
```

### 2. Initialize Training Pipeline

```bash
curl -X POST http://localhost:8080/masumi/training/initialize \
  -H "Content-Type: application/json" \
  -d '{}'
```

**Response:**
```json
{
  "status": "initialized",
  "pipeline_id": "pipeline-2025-11-30T...",
  "total_steps": 9,
  "config_parameters": 150
}
```

### 3. Run Training Pipeline

```bash
curl -X POST http://localhost:8080/masumi/training/run/pipeline-2025-11-30T... \
  -H "Content-Type: application/json"
```

**Response:**
```json
{
  "status": "started",
  "pipeline_id": "pipeline-2025-11-30T...",
  "message": "Training pipeline started in background"
}
```

### 4. Make AI Prediction

```bash
curl -X POST http://localhost:8080/masumi/predict \
  -H "Content-Type: application/json" \
  -d '{
    "wallet_address": "addr_...",
    "features": {
      "tx_count_24h": 10,
      "total_value_24h": 500000,
      "largest_value_24h": 100000,
      "std_value_24h": 50000,
      "unique_counterparts_24h": 5,
      "entropy_of_destinations": 2.3,
      "share_of_daily_volume": 0.05
    },
    "include_explanation": true,
    "include_shap": true
  }'
```

**Response:**
```json
{
  "status": "success",
  "prediction": {
    "status": "success",
    "risk_score": 0.35,
    "anomaly_score": -0.12,
    "anomaly_flag": 1,
    "explanation": {...},
    "shap_values": {...}
  }
}
```

### 5. Assess Data Quality

```bash
curl -X POST http://localhost:8080/masumi/data/quality \
  -H "Content-Type: application/json" \
  -d '{
    "records": [...],
    "missing_values": 5,
    "outliers": 2
  }'
```

---

## ðŸ”„ Training Pipeline Execution Flow

```
1. Initialize Pipeline
   â””â”€ Load config from AITrainingConfig
   â””â”€ Create AIModelTrainingPipeline object
   â””â”€ Return pipeline_id

2. Run Pipeline (9 Steps)
   â”œâ”€ Step 1: Data Loading
   â”‚  â””â”€ Fetch from Blockfrost (max_blocks, tx_count_per_address)
   â”‚  â””â”€ Output: Raw transactions JSON
   â”œâ”€ Step 2: Feature Engineering
   â”‚  â””â”€ Volume features (24h, 7d, 30d windows)
   â”‚  â””â”€ Entropy features (Shannon, RÃ©nyi)
   â”‚  â””â”€ Daily aggregation
   â”‚  â””â”€ Output: features.csv
   â”œâ”€ Step 3: Graph Analysis
   â”‚  â””â”€ Build transaction graph
   â”‚  â””â”€ Compute metrics (degree, centrality, clustering)
   â”‚  â””â”€ Optional: Community detection
   â”‚  â””â”€ Output: graph_features.csv
   â”œâ”€ Step 4: Preprocessing
   â”‚  â””â”€ Handle missing values
   â”‚  â””â”€ Scale features
   â”‚  â””â”€ Select features
   â”œâ”€ Step 5: Anomaly Detection
   â”‚  â””â”€ Train IsolationForest (300 estimators, contamination=0.1)
   â”‚  â””â”€ Train OneClassSVM (rbf kernel, nu=0.1)
   â”‚  â””â”€ Train LOF (20 neighbors)
   â”‚  â””â”€ Ensemble voting
   â”‚  â””â”€ Output: anomaly_results.csv
   â”œâ”€ Step 6: Risk Scoring
   â”‚  â””â”€ Train RandomForest (200 estimators, max_depth=20)
   â”‚  â””â”€ Split: 80% train, 20% test
   â”‚  â””â”€ Save model: randomforest.pkl
   â”œâ”€ Step 7: Model Evaluation
   â”‚  â””â”€ Compute ROC-AUC, precision/recall, F1
   â”‚  â””â”€ Feature importance
   â”‚  â””â”€ 5-fold cross-validation
   â”œâ”€ Step 8: SHAP Explanation
   â”‚  â””â”€ Generate SHAP values (TreeExplainer)
   â”‚  â””â”€ Save SHAP artifacts
   â”‚  â””â”€ Per-address contributions
   â”‚  â””â”€ Output: shap_summary.csv, per_address.json
   â””â”€ Step 9: Export
      â””â”€ Export models (PKL)
      â””â”€ Export data (CSV, JSON)
      â””â”€ Export predictions
      â””â”€ Generate report
```

---

## ðŸ“Š Parameter Mapping to Source Code

| Parameter Category | Source File | Key Functions |
|---|---|---|
| **Data Loading** | `data_pipeline.py` | `fetch_transactions_for_address()`, `build_features_from_addresses()` |
| **Feature Engineering** | `feature_engineering.py` | `load_transactions()`, `aggregate_volume_features()`, `compute_entropy_features()` |
| **Daily Aggregation** | `feature_engineering.py` | `aggregate_daily_stats()`, `compute_rolling_stats()` |
| **Graph Features** | `graph_features.py` | `build_counterparty_edges()`, `compute_graph_metrics()`, `detect_communities()` |
| **Anomaly Detection** | `ml_pipeline.py` | `run_models()` with IsolationForest, OneClassSVM, LOF |
| **Risk Scoring** | `ml_pipeline.py` | RandomForest training, evaluation |
| **Inference** | `inference.py` | `load_models()`, `detect_anomaly()`, `predict_risk()` |
| **SHAP** | `shap_explain.py` | `compute_shap_values()`, `save_shap_artifacts()` |
| **Narratives** | `narrative_explainer.py` | Human-readable explanations |
| **Live Inference** | `live_pipeline.py` | Real-time batch processing |

---

## ðŸ”Œ Integration Points

### Orchestrator â†” AI Model Agent

```python
# Via router.py routing:
route_request(registry, {
    "workflow": "ai_predict",
    "payload": {
        "wallet_address": "addr_...",
        "features": {...},
        "include_explanation": True,
        "include_shap": True
    }
})

# Via workflow: "ai_train"
route_request(registry, {
    "workflow": "ai_train",
    "payload": {
        "config": AITrainingConfig.to_dict()
    }
})
```

### AI Model Agent Execution

```python
# In ai_model_agent.py:
1. Load models from cache or disk
   â””â”€ isolationforest.pkl
   â””â”€ randomforest.pkl
   â””â”€ SHAP TreeExplainer

2. Run prediction:
   â””â”€ anomaly_score = iso_model.decision_function(features)
   â””â”€ risk_prob = rf_model.predict_proba(features)[0][1]
   â””â”€ shap_vals = explainer.shap_values(features)

3. Return standardized response:
   {
     "status": "success",
     "risk_score": 0.35,
     "anomaly_score": -0.12,
     "anomaly_flag": 1,
     "explanation": {...},
     "shap_values": {...},
     "inference_time_ms": 45.2
   }
```

---

## ðŸ“‹ Configuration Hierarchy

```
config.yaml (static)
    â”‚
    â”œâ”€â†’ agents: [payment, compliance, ai_model]
    â”‚
    â”œâ”€â†’ training: {...}  # Static training defaults
    â”‚
    â””â”€â†’ workflows: {settle, ai_predict, ai_train, ...}

AITrainingConfig (Pydantic)
    â”‚
    â”œâ”€â†’ Isolation Forest: 300 estimators, 0.1 contamination
    â”œâ”€â†’ OneClassSVM: rbf kernel, nu=0.1
    â”œâ”€â†’ LOF: 20 neighbors
    â”œâ”€â†’ Random Forest: 200 estimators, max_depth=20
    â”œâ”€â†’ SHAP: tree explainer, save artifacts
    â”œâ”€â†’ Graph: degree, centrality, clustering
    â”œâ”€â†’ Features: 24h, 7d, 30d windows + entropy + daily
    â””â”€â†’ Data: Blockfrost config + cache settings
```

---

## ðŸ”§ Running the System

### 1. Start Masumi Orchestrator (Port 8080)

```bash
cd masumi/orchestrator
uvicorn app:app --port 8080 --reload
```

**Expected Output:**
```
âœ… Registered 3 agents
INFO:     Uvicorn running on http://0.0.0.0:8080
```

### 2. Start AI Model Agent (Port 8083)

```bash
cd masumi/orchestrator
uvicorn ai_model_agent:app --port 8083 --reload
```

**Expected Output:**
```
INFO:     Uvicorn running on http://0.0.0.0:8083
âœ… Models loaded successfully
```

### 3. Verify All Services

```bash
curl http://localhost:8080/masumi/health
curl http://localhost:8080/masumi/agents
curl http://localhost:8080/masumi/stats
```

---

## ðŸ“š Key Classes & Their Relationships

```
AITrainingConfig
â”œâ”€â”€ Defines all 150+ parameters
â”œâ”€â”€ Validates via Pydantic
â””â”€â”€ Exports to dict/YAML

AIModelTrainingPipeline
â”œâ”€â”€ Tracks 9 training steps
â”œâ”€â”€ Stores config reference
â”œâ”€â”€ Records metrics/errors
â””â”€â”€ Manages lifecycle

AIModelTrainingStep
â”œâ”€â”€ Individual step execution
â”œâ”€â”€ Records timing (start/end)
â”œâ”€â”€ Captures I/O data
â””â”€â”€ Stores metrics

AIModelPredictionRequest/Response
â”œâ”€â”€ Standardized I/O schema
â”œâ”€â”€ Includes SHAP/explanations
â””â”€â”€ Tracks inference time

TrainingDataQuality
â”œâ”€â”€ Validates data completeness
â”œâ”€â”€ Detects outliers/missing
â””â”€â”€ Scores overall quality

ModelPerformanceMetrics
â”œâ”€â”€ ROC-AUC, precision, recall
â”œâ”€â”€ Confusion matrix
â”œâ”€â”€ Feature importance
â””â”€â”€ Training/test metrics
```

---

## ðŸŽ¯ Next Steps

1. **Deploy AI Model Agent** - Ensure it's running on port 8083
2. **Test Endpoints** - Use curl/Postman to verify
3. **Load Training Data** - Place data in `agents/ai_model/data/`
4. **Initialize Pipeline** - Create pipeline with POST `/masumi/training/initialize`
5. **Run Training** - Execute with POST `/masumi/training/run/{pipeline_id}`
6. **Monitor Progress** - Check status with GET `/masumi/training/pipeline/{pipeline_id}`
7. **Make Predictions** - Use POST `/masumi/predict` with trained models

---

## ðŸ†˜ Troubleshooting

| Issue | Solution |
|---|---|
| "Agent not found" | Ensure config.yaml has all 3 agents registered |
| "Models not loaded" | Check `agents/ai_model/models/*.pkl` exist |
| "Training stuck" | Check agent logs, increase timeout in router |
| "SHAP errors" | Ensure features match BASE_FEATURES in inference.py |
| "Config validation fails" | Validate against AITrainingConfig schema |

---

## ðŸ“– Documentation Files

- `MONOREPO_CANVAS.md` - Full system architecture
- `ai_training_params.py` - Complete parameter schema
- `models.py` - Data models for training/inference
- `ai_model_agent.py` - Agent implementation
- `router.py` - Workflow routing logic
- `app.py` - Orchestrator endpoints
- `config.yaml` - Agent & workflow configuration

